(314584,)
(29404, 784)
(29404, 1)
(30596, 784)
(30596, 1)
(60000, 784)
(60000, 1)
shapes
(200, 784)
(200, 1)
(157000,)
(10, 200)
(10, 1)
(2010,)
seperating
(1000, 784)
(1000, 10)
Initial cost function value is 
50.119911353756144
checking_grad
(30596, 10)
(30596, 200)
2.498098921318247
Finding theta_2 weights that optimize the soft max cost function
('lambda = ', 10)
('beta= ', 0.3)
('rHO = ', 0.05)
(30596, 10)
(30596, 200)
(30596, 10)
(30596, 200)
(30596, 10)
(30596, 200)
(30596, 10)
(30596, 200)
optimal cost function is
2.292346528809116
(60000, 1)
Percentages each number was guessed correctly using a neural network
(10, 1)
[[  0.]
 [100.]
 [  0.]
 [  0.]
 [  0.]
 [  0.]
 [  0.]
 [  0.]
 [  0.]
 [  0.]]
900348 0.09900353 0.09900347]
[0.10092125 0.10118865 0.10095065 0.10101974 0.10090084 0.0990038
 0.0990038  0.09900374 0.0990038  0.09900373]
[0.10094868 0.10117812 0.10100053 0.10102278 0.10088658 0.09899269
 0.09899269 0.09899263 0.09899268 0.09899262]
[0.10090948 0.10130555 0.10093596 0.10099868 0.10089199 0.0989917
 0.0989917  0.09899164 0.09899169 0.09899162]
[0.10092904 0.10119276 0.10093766 0.10099322 0.10090215 0.09900906
 0.09900906 0.099009   0.09900905 0.09900898]
[0.10094151 0.10118695 0.1009451  0.10099232 0.10091689 0.09900347
 0.09900348 0.09900342 0.09900347 0.0990034 ]
[0.10093655 0.10118942 0.10094346 0.1010044  0.10090082 0.0990051
 0.0990051  0.09900504 0.09900509 0.09900502]
[0.1009405  0.10118959 0.10096661 0.1009983  0.10089623 0.09900178
 0.09900178 0.09900172 0.09900177 0.09900171]
[0.1010091  0.10117622 0.10095882 0.10099356 0.10089726 0.09899303
 0.09899304 0.09899298 0.09899303 0.09899296]
Soft_Max_Classifier.py:353: RuntimeWarning: invalid value encountered in divide
  scaling_down_neural[s] = 100.0*(scaling_down_neural[s] / float(total_digit_count_test_set[s]) )
Percentages each number was guessed correctly using a neural network
(10, 1)
[[  0.]
 [100.]
 [  0.]
 [  0.]
 [  0.]
 [ nan]
 [ nan]
 [ nan]
 [ nan]
 [ nan]]
